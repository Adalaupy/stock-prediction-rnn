{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NWvH1AuttvQH"
   },
   "source": [
    "# Define Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1762827408344,
     "user": {
      "displayName": "YU L",
      "userId": "08814620792587341017"
     },
     "user_tz": -480
    },
    "id": "KAzLTfu2zFgo"
   },
   "outputs": [],
   "source": [
    "Epochs = 100\n",
    "Batch_Size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FTLq1BFKtwzT"
   },
   "source": [
    "# Call Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 89,
     "status": "ok",
     "timestamp": 1762827408442,
     "user": {
      "displayName": "YU L",
      "userId": "08814620792587341017"
     },
     "user_tz": -480
    },
    "id": "EyrhbDar0zmf"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout,RepeatVector, TimeDistributed, LayerNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from datetime import datetime\n",
    "import os\n",
    "import shutil\n",
    "import joblib\n",
    "import gc\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1762827408442,
     "user": {
      "displayName": "YU L",
      "userId": "08814620792587341017"
     },
     "user_tz": -480
    },
    "id": "2ZrbW1h78YEy"
   },
   "outputs": [],
   "source": [
    "%run Common.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m-f6OPS4t3m8"
   },
   "source": [
    "# Define Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AjH_Bsmjt1E8"
   },
   "source": [
    "## Create dataset Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1762827408442,
     "user": {
      "displayName": "YU L",
      "userId": "08814620792587341017"
     },
     "user_tz": -480
    },
    "id": "zPrr-x9f-jK6"
   },
   "outputs": [],
   "source": [
    "def Create_Seq(data, p , Horizon):\n",
    "\n",
    "    X, y , dates = [],[] , []\n",
    "\n",
    "    for i in range( len(data) - p - Horizon + 1 ):\n",
    "\n",
    "        X_data = data[i: i + p]\n",
    "        y_data = data[i + p : i + p + Horizon]\n",
    "\n",
    "        X.append(X_data['Scaled_Close'].values.reshape(-1,1) )\n",
    "        y.append(y_data['Scaled_Close'].values.reshape(-1,1))\n",
    "\n",
    "        dates.append(y_data['Date'].tolist())\n",
    "\n",
    "\n",
    "        # print(f'Use {X_data.iloc[0]['Date'].date()} to {X_data.iloc[-1]['Date'].date()} to predict stock price from {y_data.iloc[0]['Date'].date()} - {y_data.iloc[-1]['Date'].date()}')\n",
    "\n",
    "    return np.array(X), np.array(y), dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3pWJwoyEt599"
   },
   "source": [
    "## Build RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1762827408443,
     "user": {
      "displayName": "YU L",
      "userId": "08814620792587341017"
     },
     "user_tz": -480
    },
    "id": "XqfE3BOLJRfq"
   },
   "outputs": [],
   "source": [
    "def Build_Model( Horizon, p, units = 64, dropout = 0.2, layers = 2):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # -------------- Encoder --------------\n",
    "    for i in range(layers):\n",
    "\n",
    "        model.add(LSTM(units , \n",
    "                       input_shape=(p, 1) if i == 0 else None,\n",
    "                       return_sequences = True))\n",
    "        \n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "\n",
    "    \n",
    "    # -------------- Bottleneck --------------\n",
    "    model.add(LSTM(units, return_sequences=False))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    \n",
    "    \n",
    "    # -------------- RepeatVector --------------\n",
    "    model.add(RepeatVector(Horizon))\n",
    "\n",
    "\n",
    "    # -------------- Decoder --------------    \n",
    "    for _ in range(layers):\n",
    "        model.add(LSTM(units, return_sequences = True))\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(TimeDistributed(Dense(1)))\n",
    "\n",
    "\n",
    "    # ---------- Compile ----------\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='mse',                # faster + better for scaled prices\n",
    "        metrics=['mae']\n",
    "    )\n",
    "\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N8b8adkPt8Dc"
   },
   "source": [
    "## Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1762827408444,
     "user": {
      "displayName": "YU L",
      "userId": "08814620792587341017"
     },
     "user_tz": -480
    },
    "id": "D7_zAUi5JL37"
   },
   "outputs": [],
   "source": [
    "def Fit_Model(  X_train, y_train , X_test, y_test , Horizon, p , patience = 35):\n",
    "\n",
    "    model = Build_Model( Horizon , p )\n",
    "\n",
    "    early_stop = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience = patience,\n",
    "        restore_best_weights=True,\n",
    "        min_delta=1e-6,\n",
    "        verbose = 0\n",
    "    )\n",
    "\n",
    "\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_test, y_test),\n",
    "        epochs=Epochs,\n",
    "        batch_size = Batch_Size,\n",
    "        callbacks=[early_stop],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kVlJWGFit9Z7"
   },
   "source": [
    "## Handle Predict Result and Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 48,
     "status": "ok",
     "timestamp": 1762827408493,
     "user": {
      "displayName": "YU L",
      "userId": "08814620792587341017"
     },
     "user_tz": -480
    },
    "id": "Oh5_hoFyKmkj"
   },
   "outputs": [],
   "source": [
    "def Predict_Result_Agg(predicts,dates):\n",
    "\n",
    "    results = []\n",
    "    order_counter = 1\n",
    "\n",
    "    for predictList, dateList in zip(predicts,dates ):\n",
    "\n",
    "        for predict, date in zip(predictList , dateList):\n",
    "\n",
    "            result = {\"Date\": date.date(), \"Predict\" : predict[0], \"Order\": order_counter}\n",
    "\n",
    "            results.append(result)\n",
    "            order_counter += 1\n",
    "\n",
    "\n",
    "    result_df = pd.DataFrame(results)\n",
    "    \n",
    "    # result_df.to_csv('result.csv', index=False)\n",
    "\n",
    "\n",
    "    aggregated_result = result_df.copy()\n",
    "    aggregated_result = aggregated_result.groupby(['Date']).agg(\n",
    "        count=('Predict', 'count'),\n",
    "        mean=('Predict', 'mean'),\n",
    "        Predict_Order_Sum=('Predict', lambda x: (x * result_df.loc[x.index, 'Order']).sum()),\n",
    "        Order_Sum=('Order', 'sum')\n",
    "    ).reset_index()\n",
    "\n",
    "\n",
    "    # Weighted Predict\n",
    "    aggregated_result['Weighted_Mean'] = aggregated_result['Predict_Order_Sum'] / aggregated_result['Order_Sum']\n",
    "    aggregated_result = aggregated_result.drop(columns=['Predict_Order_Sum', 'Order_Sum'])\n",
    "\n",
    "\n",
    "    # Rename + Scaler\n",
    "    aggregated_result = aggregated_result.rename(columns={'mean': 'Close_Pred'})\n",
    "    aggregated_result[\"Close_Pred\"] = scaler.inverse_transform(aggregated_result[['Close_Pred']])\n",
    "\n",
    "\n",
    "    result_df = result_df.rename(columns={'Predict': 'Close_Pred'})\n",
    "    result_df[\"Close_Pred\"] = scaler.inverse_transform(result_df[['Close_Pred']])\n",
    "\n",
    "    \n",
    "    \n",
    "    return aggregated_result, result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWiumaBluE-l"
   },
   "source": [
    "## Overall RNN Process for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 44,
     "status": "ok",
     "timestamp": 1762827408495,
     "user": {
      "displayName": "YU L",
      "userId": "08814620792587341017"
     },
     "user_tz": -480
    },
    "id": "jon3AGLTH4-t"
   },
   "outputs": [],
   "source": [
    "def RNN(data, p , t , Horizon):\n",
    "\n",
    "\n",
    "    X, y, dates = Create_Seq(data, p , Horizon)\n",
    "\n",
    "    # Split Data to training and Validation dataset\n",
    "    v = int(len(X) * t)\n",
    "    X_train, X_test = X[:v], X[v:]\n",
    "    y_train, y_test = y[:v], y[v:]\n",
    "    d_train, d_test = dates[:v], dates[v:]\n",
    "\n",
    "\n",
    "    # Fit Model and predict validation dataset\n",
    "    model = Fit_Model(  X_train, y_train , X_test, y_test , Horizon, p)\n",
    "\n",
    "    predicts = model.predict(X_test)\n",
    "    agg_df,indi_df = Predict_Result_Agg(predicts,d_test)\n",
    "\n",
    "\n",
    "    # Join 2 df\n",
    "    agg_df['Date'] = pd.to_datetime(agg_df['Date'])\n",
    "    merged_df = pd.merge(data[['Code', 'Date', 'Close']], agg_df[['Date', 'Close_Pred']], on='Date', how='left')\n",
    "\n",
    "\n",
    "    indi_df['Date'] = pd.to_datetime(indi_df['Date'])\n",
    "    indi_df = pd.merge( indi_df[['Date', 'Close_Pred' , 'Order']] , data[['Code', 'Date', 'Close']] , on='Date', how='left')\n",
    "    indi_df = indi_df[['Date', 'Close' , 'Close_Pred' , 'Order']]\n",
    "    \n",
    "\n",
    "    # Get Mean Absolute Error\n",
    "    merged_df_cleaned = merged_df.dropna(subset=['Close_Pred']).copy()\n",
    "    mae = mean_absolute_error(merged_df_cleaned['Close'], merged_df_cleaned['Close_Pred'])\n",
    "\n",
    "    mae_loss = tf.keras.losses.MeanAbsoluteError()\n",
    "    mae_model = float(mae_loss(indi_df['Close'], indi_df['Close_Pred']).numpy())\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    print(f'Mean Absolute Error: {round(mae,3)} + {round(mae_model,3)}')\n",
    "\n",
    "\n",
    "    # Omit the old data for better presentation\n",
    "    data_df = merged_df.tail( len(predicts) + 50)\n",
    "\n",
    "\n",
    "    return mae,mae_model, model, data_df, indi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 0,
     "status": "ok",
     "timestamp": 1762827408496,
     "user": {
      "displayName": "YU L",
      "userId": "08814620792587341017"
     },
     "user_tz": -480
    },
    "id": "MlrOk0au-7v4"
   },
   "outputs": [],
   "source": [
    "def Main_RNN(data, Horizon, min_p, gp):\n",
    "\n",
    "    Best_Result_List = []\n",
    "\n",
    "    Code = data['Code'].drop_duplicates().values[0]\n",
    "\n",
    "\n",
    "    scaler = MinMaxScaler((0,1))\n",
    "    scaler.fit(data[['Close']])\n",
    "    data['Scaled_Close'] = scaler.transform(data[['Close']])\n",
    "\n",
    "    for i in range(6,9):\n",
    "\n",
    "        t = i / 10\n",
    "        max_p = int(len(data) / 4)\n",
    "\n",
    "        best_result = None\n",
    "\n",
    "\n",
    "        for p in range(min_p, max_p, int((max_p - min_p)/gp)):\n",
    "\n",
    "\n",
    "            print(\"=\" * 100 )\n",
    "            print(f\"Training for Seq_len = {p}/{max_p} with validation fraction = {t} \")\n",
    "\n",
    "            mae,mae_model, model, data_df = RNN(data, p , t , Horizon)\n",
    "\n",
    "            output_df = data_df.copy()\n",
    "\n",
    "            current = {\"mae\":mae, \"mae_model\":mae_model , \"model\": model, \"df\": output_df, \"t\": t, \"p\": p}\n",
    "\n",
    "            if best_result == None:\n",
    "\n",
    "                best_result = current\n",
    "\n",
    "\n",
    "            elif mae_model < best_result['mae']:\n",
    "\n",
    "                best_result = current\n",
    "\n",
    "\n",
    "        Best_Result_List.append(best_result)\n",
    "\n",
    "\n",
    "    # BackUp Previous Folder\n",
    "    ThisFolder = f\"{RNN_PATH}/{Code}\"\n",
    "    CurrFolder = f\"{ThisFolder}/Current\"\n",
    "    BackUpName = f\"{ThisFolder}/{datetime.now().strftime(\"%Y%m%d\")}\"\n",
    "    fileName   = f'RNN_{Code}'\n",
    "\n",
    "    if os.path.exists(CurrFolder):\n",
    "\n",
    "        os.rename(CurrFolder , BackUpName)\n",
    "\n",
    "    os.makedirs(CurrFolder)\n",
    "\n",
    "\n",
    "    # Create directory if it doesn't exist\n",
    "    if not os.path.exists(CurrFolder):\n",
    "    \n",
    "        os.makedirs(CurrFolder)\n",
    "\n",
    "\n",
    "    # Store Scaler\n",
    "    joblib.dump(scaler, f\"{CurrFolder}/{fileName}_Scaler.pkl\")\n",
    "\n",
    "\n",
    "    # Save Best Result\n",
    "    for id in range(len(Best_Result_List)):\n",
    "\n",
    "        display_id = id + 1\n",
    "\n",
    "        best = Best_Result_List[id]\n",
    "\n",
    "\n",
    "        title = f'Id: {display_id}\\nRNN({best['p']} with training Fraction = {best['t']})'\n",
    "        name = f'Current/RNN({best['p']})_{display_id}'\n",
    "\n",
    "\n",
    "        Plot_Result(best[\"df\"], title, Code, model = 'RNN' , name = name , IsSave = True)\n",
    "\n",
    "\n",
    "        # Save Model\n",
    "        model.save(f\"{CurrFolder}/{fileName}_{display_id}.keras\")\n",
    "\n",
    "\n",
    "        # Save to csv\n",
    "        best[\"df\"]['p'] = p\n",
    "        best[\"df\"]['Train_Fraction'] = t\n",
    "        best['df']['Horizon'] = Horizon\n",
    "        best[\"df\"].to_csv(f\"{CurrFolder}/{fileName}_{display_id}.csv\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNYZTpdAwQB8Z8MKy8OjDPB",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
